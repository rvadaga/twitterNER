# written by Xeuzhe Max
# link: https://github.com/XuezheMax/LasagneNLP

from __future__ import print_function
import numpy as np
from collection import Collection
import utils
import constants as const
from gensim.models.word2vec import Word2Vec
import gzip
import sys
import tensorflow as tf


def get_logger(LOG_FILE):
    global logger
    logger = utils.get_logger("LoadData", LOG_FILE)


def readDataFromFile(path,
                     word_collection,
                     label_collection):
    """Reads data from the given path.

    Finds the words  and labels in each sentence and adds them to the 
    Collection objects, which assigns the indices.

    Returns an a list of list of word and label indices

    Args:
        path: path to the file to be loaded
        word_collection: collection of words
        label_collection: collection of labels

    Returns:
        word_sentences: list of list of words in the given file
        label_sentences: list of list of labels in the given file
        word_index_sentences: list of list of word indices. Words for
            word indices can be obtained from word collection object
        label_index_sentences: list of list of label indices. Labels
            corresponding to the label indices can be obtained from
            label collection object

    """
    word_sentences = []
    label_sentences = []

    word_index_sentences = []
    label_index_sentences = []

    words = []
    labels = []

    word_ids = []
    label_ids = []

    num_tokens = 0
    with open(path) as file:
        for line in file:
            line.decode('utf-8')
            if line.strip() == "":
                if 0 < len(words) <= const.MAX_LENGTH:
                    word_sentences.append(words[:])
                    label_sentences.append(labels[:])

                    word_index_sentences.append(word_ids[:])
                    label_index_sentences.append(label_ids[:])

                    num_tokens += len(words)
                else:
                    if len(words) != 0:
                        logger.info("ignore sentence with length %d" %
                                    (len(words)))

                words = []
                labels = []

                word_ids = []
                label_ids = []
            else:
                tokens = line.strip().split()
                word = tokens[const.WORD_COLUMN]
                label = tokens[const.LABEL_COLUMN]

                words.append(word)
                labels.append(label)

                word_id = word_collection.get_index(word)
                label_id = label_collection.get_index(label)

                word_ids.append(word_id)
                label_ids.append(label_id)

    if 0 < len(words) <= const.MAX_LENGTH:
        word_sentences.append(words[:])
        label_sentences.append(labels[:])

        word_index_sentences.append(word_ids[:])
        label_index_sentences.append(label_ids[:])

        num_tokens += len(words)
    else:
        if len(words) != 0:
            logger.warning("Ignore sentence with length %d" % (len(words)))

    logger.info("#sentences: %d, #tokens: %d" % (len(word_sentences),
                                                 num_tokens))
    return (word_sentences,
            label_sentences,
            word_index_sentences,
            label_index_sentences)


def readOrthDataFromFile(path,
                         word_collection):
    """
    Read orthographic words from file and map them to IDs.
    Returns a list of list of word IDs for the file.

    Input:
        path:
                path to the
                file to be loaded
        orth_word_collection:
                collection of words
    Returns:
        word_sentences:
                list of list of words in the file,
                where inner list is list of words in
                a sentence
        word_index_sentences:
                list of list of word indices, where each
                inner list holds indices of each word (from word_collection)
                in a sentence
    """
    word_sentences = []
    word_index_sentences = []
    words = []
    word_ids = []
    num_tokens = 0
    path = path + "_orth"
    with open(path) as file:
        for line in file:
            line.decode('utf-8')
            if line.strip() == "":
                if 0 < len(words) <= const.MAX_LENGTH:
                    word_sentences.append(words[:])
                    word_index_sentences.append(word_ids[:])
                    num_tokens += len(words)
                else:
                    if len(words) != 0:
                        logger.info("ignore sentence with length %d" %
                                    (len(words)))

                words = []

                word_ids = []
            else:
                tokens = line.strip().split()
                word = tokens[0]

                words.append(word)
                word_id = word_collection.get_index(word)
                word_ids.append(word_id)

    if 0 < len(words) <= const.MAX_LENGTH:
        word_sentences.append(words[:])
        word_index_sentences.append(word_ids[:])
        num_tokens += len(words)
    else:
        if len(words) != 0:
            logger.info("ignore sentence with length %d" % (len(words)))

    logger.info("#sentences: %d, #tokens: %d" % (len(word_sentences),
                                                 num_tokens))
    return word_sentences, word_index_sentences


def get_sequence_lengths(word_sentences):
    max_len = 0
    lengths = []
    for sentence in word_sentences:
        length = len(sentence)
        lengths.append(length)
        if length > max_len:
            max_len = length
    return np.array(lengths), max_len


def generate_character_data(
        sentences_train,
        sentences_dev,
        sentences_test,
        max_sent_length,
        collectionType,
        char_embedd_dim=30):
    """
    generate data for charaters
    :param sentences_train:
    :param sentences_dev:
    :param sentences_test:
    :param max_sent_length:
    :return: C_train, C_dev, C_test, char_embedd_table
    """

    def get_character_indexes(sentences):
        index_sentences = []
        max_length = 0
        for words in sentences:
            index_words = []
            for word in words:
                index_chars = []
                if len(word) > max_length:
                    max_length = len(word)

                for char in word[:const.MAX_CHAR_LENGTH]:
                    char_id = char_collection.get_index(char)
                    index_chars.append(char_id)

                index_words.append(index_chars)
            index_sentences.append(index_words)
        return index_sentences, max_length

    def construct_tensor_char(index_sentences):
        C = np.empty([len(index_sentences), max_sent_length, max_char_length],
                     dtype=np.int32)
        if collectionType == "char":
            word_end_id = char_collection.get_index(const.WORD_END)
        else:
            word_end_id = char_collection.get_index(const.ORTH_WORD_END)

        for i in range(len(index_sentences)):
            words = index_sentences[i]
            sent_length = len(words)
            for j in range(sent_length):
                chars = words[j]
                char_length = len(chars)
                for k in range(char_length):
                    cid = chars[k]
                    C[i, j, k] = cid
                # fill index of word end after the end of word
                C[i, j, char_length:] = word_end_id
            # Zero out C after the end of the sentence
            C[i, sent_length:, :] = 0
        # C = np.reshape(C, (len(index_sentences),
                       # max_sent_length * max_char_length))
        return C

    def build_char_embedd_table():
        scale = np.sqrt(3.0 / char_embedd_dim)
        char_embedd_table = np.random.uniform(
            -scale,
            scale,
            [char_collection.size(), char_embedd_dim]).astype(
                np.float32)
        return char_embedd_table

    char_collection = Collection(collectionType)
    if collectionType == "char":
        char_collection.get_index(const.WORD_END)
    else:
        char_collection.get_index(const.ORTH_WORD_END)

    index_sentences_train, max_char_length_train = \
        get_character_indexes(sentences_train)
    index_sentences_dev, max_char_length_dev = \
        get_character_indexes(sentences_dev)
    index_sentences_test, max_char_length_test = \
        get_character_indexes(sentences_test)

    # close character collection
    char_collection.close()
    logger.info(collectionType +
                ": char collection size: %d" %
                (char_collection.size() - 1))

    max_char_length = min(
        const.MAX_CHAR_LENGTH,
        max(max_char_length_train,
            max_char_length_dev,
            max_char_length_test))
    logger.info(collectionType +
                ": maximum char length in training set: %d" %
                max_char_length_train)
    logger.info(collectionType +
                ": maximum char length in dev set: %d" %
                max_char_length_dev)
    logger.info(collectionType +
                ": maximum char length in test set: %d" %
                max_char_length_test)
    logger.info(collectionType +
                ": maximum char length used for training: %d" %
                max_char_length)

    # fill character tensor
    C_train = construct_tensor_char(index_sentences_train)
    C_dev = construct_tensor_char(index_sentences_dev)
    C_test = construct_tensor_char(index_sentences_test)

    return (C_train, C_dev, C_test, build_char_embedd_table())


def build_embedd_table(word_collection, word_emb_dict, word_emb_dim, caseless):
    scale = np.sqrt(3.0 / word_emb_dim)
    embedd_table = np.empty([word_collection.size(), word_emb_dim],
                            dtype=np.float32)
    embedd_table[word_collection.default_index, :] = \
        np.random.uniform(-scale, scale, [1, word_emb_dim])
    for word, index in word_collection.iteritems():
        ww = word.lower() if caseless else word
        embedd = word_emb_dict[ww] if ww in word_emb_dict else \
            np.random.uniform(-scale, scale, [1, word_emb_dim])
        embedd_table[index, :] = embedd
    return embedd_table


def load_data(args):
    """
    Reads the words, embeddings from files
    and prepares the dataset for training

    Input:
        args:
            cmd line args

    Returns:
        data:
            1. word indices in train, dev and test datasets
            2. word and label collection i.e.,
               idx-instance and instance-idx mapping
            3. word, orth_word and char embedding tables
    """
    def construct_tensor_fine_tune(word_index_sentences,
                                   label_index_sentences):
        X = np.empty([len(word_index_sentences), max_length], dtype=np.int32)
        Y = np.empty([len(word_index_sentences), max_length], dtype=np.int32)
        mask = np.zeros([len(word_index_sentences), max_length],
                        dtype=np.int32)

        for i in range(len(word_index_sentences)):
            word_ids = word_index_sentences[i]
            label_ids = label_index_sentences[i]
            length = len(word_ids)
            for j in range(length):
                wid = word_ids[j]
                label = label_ids[j]
                X[i, j] = wid
                Y[i, j] = label - 1

            # Zero out X after the end of the sequence
            X[i, length:] = 0
            # Copy the last label after the end of the sequence
            Y[i, length:] = Y[i, length - 1]
            # Make the mask for this sample 1 within the range of length
            mask[i, :length] = 1
        return (X, Y, mask)

    def construct_orth_tensor_fine_tune(orth_word_index_sentences):
        X = np.empty([len(orth_word_index_sentences), max_length],
                     dtype=np.int32)

        for i in range(len(orth_word_index_sentences)):
            orth_word_ids = orth_word_index_sentences[i]
            length = len(orth_word_ids)
            for j in range(length):
                wid = orth_word_ids[j]
                X[i, j] = wid

            # Zero out X after the end of the sequence
            X[i, length:] = 0
        return X

    def generateDatasetFineTune():
        """
        generate data to be feed to the network
        Returns:
            data:
                1. word, orth_word and char indices of train, dev and
                   test datasets
                2. word, orth_word embedding tables
                3. char and orth_char embedding tables
        """
        word_emb_dict, word_emb_dim, caseless = loadEmbeddingsFromFile(
            emb_to_use,
            emb_path,
            word_collection,
            logger)
        logger.info("Word embedding dimension: %d, case: %s" %
                    (word_emb_dim, not caseless))
        orth_word_emb_dict = \
            randomlyInitialiseOrthEmbeddings(orth_word_collection,
                                             orth_word_emb_dim)
        logger.info("Orthographic word embedding dimension: %d" %
                    (orth_word_emb_dim))

        # find X, Y and mask for train data
        # train_data is a tuple
        train_data_word = construct_tensor_fine_tune(
            word_index_sentences_train,
            label_index_sentences_train)

        # find X for orth train data
        train_data_orth = construct_orth_tensor_fine_tune(
            orth_word_index_sentences_train)

        # find X, Y and mask for dev data
        dev_data_word = construct_tensor_fine_tune(
            word_index_sentences_dev,
            label_index_sentences_dev)

        # find X for orth dev data
        dev_data_orth = construct_orth_tensor_fine_tune(
            orth_word_index_sentences_dev)

        # find X, Y and mask for test data
        test_data_word = construct_tensor_fine_tune(
            word_index_sentences_test,
            label_index_sentences_test)

        # find X for orth test data
        test_data_orth = construct_orth_tensor_fine_tune(
            orth_word_index_sentences_test)

        # generate char emb table
        char_data = generate_character_data(
            word_sentences_train,
            word_sentences_dev,
            word_sentences_test,
            max_length,
            "char",
            30) if args.use_char else \
            (None, None, None, None)

        # generate orth char emb table
        orth_char_data = generate_character_data(
            orth_word_sentences_train,
            orth_word_sentences_dev,
            orth_word_sentences_test,
            max_length,
            "orth_char",
            30) if args.use_char else \
            (None, None, None, None)

        # generate word emb table
        word_emb_table = build_embedd_table(
            word_collection,
            word_emb_dict,
            word_emb_dim,
            caseless)

        # generate orth word emb table
        orth_word_emb_table = build_embedd_table(
            orth_word_collection,
            orth_word_emb_dict,
            orth_word_emb_dim,
            False)

        train_data = {
            "word": train_data_word[0],
            "label": train_data_word[1],
            "mask": train_data_word[2],
            "char": char_data[0],
            "orth_word": train_data_orth,
            "orth_char": orth_char_data[0],
            "length": train_len
        }
        dev_data = {
            "word": dev_data_word[0],
            "label": dev_data_word[1],
            "mask": dev_data_word[2],
            "char": char_data[1],
            "orth_word": dev_data_orth,
            "orth_char": orth_char_data[1],
            "length": dev_len
        }
        test_data = {
            "word": test_data_word[0],
            "label": test_data_word[1],
            "mask": test_data_word[2],
            "char": char_data[2],
            "orth_word": test_data_orth,
            "orth_char": orth_char_data[2],
            "length": test_len
        }
        table = {
            "label": label_collection,
            "word": word_collection
        }
        emb = {
            "char": char_data[3],
            "orth_char": orth_char_data[3],
            "word": word_emb_table,
            "orth_word": orth_word_emb_table
        }

        return train_data, dev_data, test_data, table, emb

    # ####################
    # Start of load_data()
    # ####################

    train_path = args.wnut + "/train_dev"
    dev_path = args.wnut + "/dev_2015"
    test_path = args.wnut + "/test"
    emb_to_use = args.emb
    emb_path = args.emb_path
    fine_tune = args.fine_tune
    oov = args.oov
    orth_word_emb_dim = args.orth_emb_dim

    word_collection = Collection("word")
    label_collection = Collection("label")
    orth_word_collection = Collection("orth_word")

    # read training data
    logger.info("Reading words from train set")
    train_data = readDataFromFile(
        train_path,
        word_collection,
        label_collection)
    word_sentences_train = train_data[0]
    word_index_sentences_train = train_data[2]
    label_index_sentences_train = train_data[3]

    logger.info("Reading words from orth train set")
    train_data_orth = readOrthDataFromFile(
        train_path,
        orth_word_collection)
    orth_word_sentences_train = train_data_orth[0]
    orth_word_index_sentences_train = train_data_orth[1]

    # if oov is "random" and do not fine tune, close word_collection
    # and orth_word_collection
    if oov == "random" and not fine_tune:
        logger.warning("Closed word and orth_word collection")
        word_collection.close()
        orth_word_collection.close()

    # read dev data
    logger.info("Reading data from dev set")
    dev_data = readDataFromFile(
        dev_path,
        word_collection,
        label_collection)
    word_sentences_dev = dev_data[0]
    word_index_sentences_dev = dev_data[2]
    label_index_sentences_dev = dev_data[3]

    logger.info("Reading data from orth dev set")
    dev_data_orth = readOrthDataFromFile(
        dev_path,
        orth_word_collection)
    orth_word_sentences_dev = dev_data_orth[0]
    orth_word_index_sentences_dev = dev_data_orth[1]

    # read test data
    logger.info("Reading data from test set")
    test_data = readDataFromFile(
        test_path,
        word_collection,
        label_collection)
    word_sentences_test = test_data[0]
    word_index_sentences_test = test_data[2]
    label_index_sentences_test = test_data[3]

    logger.info("Reading data from orth test set")
    test_data_orth = readOrthDataFromFile(
        test_path,
        orth_word_collection)
    orth_word_sentences_test = test_data_orth[0]
    orth_word_index_sentences_test = test_data_orth[1]

    # close collection sets
    word_collection.close()
    label_collection.close()
    orth_word_collection.close()

    logger.info("Word collection size: %d" % (word_collection.size() - 1))
    logger.info("Label collection size: %d" % (label_collection.size() - 1))
    logger.info("Orthographic word collection size: %d" %
                (orth_word_collection.size() - 1))

    # get maximum length
    train_len, max_length_train = get_sequence_lengths(word_sentences_train)
    dev_len, max_length_dev = get_sequence_lengths(word_sentences_dev)
    test_len, max_length_test = get_sequence_lengths(word_sentences_test)
    max_length = min(const.MAX_LENGTH,
                     max(max_length_train,
                         max_length_dev,
                         max_length_test))
    logger.info("Max sentence length of training set: %d" % max_length_train)
    logger.info("Max sentence length of dev set: %d" % max_length_dev)
    logger.info("Max sentence length of test set:  %d" % max_length_test)
    logger.info("Max sentence length for training: %d" % max_length)

    if fine_tune:
        logger.info("Generating data with fine tuning")
        return generateDatasetFineTune()
    else:
        logger.info("generating data without fine tuning")
        return generateDatasetWithoutFineTune()


def loadEmbeddingsFromFile(emb_to_use,
                           emb_path,
                           word_collection,
                           emb_dim=100):
    """
    Load word embeddings from file
    Params:
        emb_to_use:
            should be one of
            "w2v", "w2v_twitter", "glove", "senna", "random"
        emb_path:
            embedding file location
        word_collection:
            word collection object
        emb_dim:
            embedding dimension to use for random embeddings

    Returns:
        embedding dict
        embedding dimension
        caseless
    """
    if emb_to_use == 'word2vec':
        # loading word2vec
        logger.info("Loading word2vec embeddings")
        word2vec = Word2Vec.load_word2vec_format(emb_path, binary=True)
        emb_dim = word2vec.vector_size
        return word2vec, emb_dim, False
    elif emb_to_use == 'glove':
        # loading GloVe
        logger.info("Loading GloVe embeddings")
        emb_dim = -1
        emb_dict = dict()
        with gzip.open(emb_path, 'r') as file:
            for line in file:
                line = line.strip()
                if len(line) == 0:
                    continue

                tokens = line.split()
                if emb_dim < 0:
                    emb_dim = len(tokens) - 1
                else:
                    assert (emb_dim + 1 == len(tokens))
                embedd = np.empty([1, emb_dim], dtype=np.float32)
                embedd[:] = tokens[1:]
                emb_dict[tokens[0]] = embedd
        return emb_dict, emb_dim, True
    elif emb_to_use == 'senna':
        # loading Senna
        logger.info("Loading Senna embeddings")
        emb_dim = -1
        emb_dict = dict()
        with gzip.open(emb_path, 'r') as file:
            for line in file:
                line = line.strip()
                if len(line) == 0:
                    continue

                tokens = line.split()
                if emb_dim < 0:
                    emb_dim = len(tokens) - 1
                else:
                    assert (emb_dim + 1 == len(tokens))
                embedd = np.empty([1, emb_dim], dtype=np.float32)
                embedd[:] = tokens[1:]
                emb_dict[tokens[0]] = embedd
        return emb_dict, emb_dim, True
    elif emb_to_use == 'random':
        # loading random embedding table
        logger.info("Loading random embeddings")
        emb_dict = dict()
        words = word_collection.get_content()
        scale = np.sqrt(3.0 / emb_dim)
        for word in words:
            emb_dict[word] = np.random.uniform(-scale,
                                               scale,
                                               [1, emb_dim])
        return emb_dict, emb_dim, False
    elif emb_to_use == "w2v_twitter":
        logger.info("Loading w2v_twitter embeddings")
        emb_dict = dict()
        with open(emb_path, "rb") as f:
            header = f.readline()
            vocab_size, emb_size = map(int, header.split())
            binary_len = np.dtype(np.float32).itemsize * emb_size
            # FIXME: change this to vocab_size later
            for idx in xrange(100):
                word = []
                while True:
                    ch = f.read(1)
                    if ch == b' ':
                        break
                    if ch != b'\n':  # ignore newlines in front of words
                        # (some binary files have newline, some don't)
                        word.append(ch)
                word = to_unicode(b''.join(word), encoding='latin-1')
                emb = np.empty([1, emb_size], dtype=np.float32)
                emb = np.fromstring(f.read(binary_len),
                                    dtype=np.float32)
                emb_dict[word] = emb
            return emb_dict, emb_size, False
    else:
        raise ValueError("Embedding should choose from [word2vec, senna]")


def randomlyInitialiseOrthEmbeddings(orth_word_collection,
                                     emb_dim=200):
    """
    load word embeddings from file
    Input:
        orth_word_collection:
            collection object containing instance-idx and
            idx2instance mapping for orth words
        emb_dim:
            embedding dimension for random embeddings
    Returns:
        emb_dict
    """
    logger.info("Initialising orth word embeddings")
    emb_dict = dict()
    words = orth_word_collection.get_content()
    scale = np.sqrt(3.0 / emb_dim)
    for word in words:
        emb_dict[word] = np.random.uniform(-scale, scale, [1, emb_dim])
    return emb_dict


if sys.version_info[0] >= 3:
    unicode = str


def to_unicode(text, errors='strict', encoding='utf8'):
    """
    Convert a string (unicode or bytestring in `encoding`),
    to bytestring in utf8.
    """
    if isinstance(text, unicode):
        return text.encode('utf8')
    # do bytestring -> unicode -> utf8 full circle, to ensure valid utf8
    return unicode(text, encoding, errors=errors).encode('utf8')


def data_generator(data, args, type, name):
    if type == "train":
        data_word = data["train_word"]
        data_orth = data["train_orth"]
        data_char = data["char"][0]
    elif type == "dev":
        data_word = data["dev_word"]
        data_orth = data["dev_orth"]
        data_char = data["char"][1]
    else:
        print("Type of data not understood")
    with tf.name_scope(name, "WNUT_NER_Data_Generator", [data, args]):
        raw_data = tf.convert_to_tensor()
